{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '*', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "[' ', '*', '^', 'ँ', 'ं', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', '॥']\n"
     ]
    }
   ],
   "source": [
    "# PAD_START = ['*']\n",
    "# PAD_NULL = [' ']\n",
    "# TOKEN_UNK\n",
    "TOKEN_PAD = ' '\n",
    "TOKEN_SOS = '*'\n",
    "TOKEN_EOS = '^'\n",
    "\n",
    "\n",
    "\n",
    "chr_eng = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "chr_hin = ['ँ', 'ं', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', '॥']\n",
    "\n",
    "eng_vocab = [TOKEN_PAD,TOKEN_SOS,TOKEN_EOS]+chr_eng\n",
    "hin_vocab = [TOKEN_PAD,TOKEN_SOS,TOKEN_EOS]+chr_hin\n",
    "\n",
    "print(eng_vocab)\n",
    "print(hin_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 71)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_vocab), len(hin_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1],\n",
       "        [10, 14],\n",
       "        [11,  3],\n",
       "        [11, 13],\n",
       "        [20, 21],\n",
       "        [ 2,  8],\n",
       "        [ 0,  8],\n",
       "        [ 0,  8],\n",
       "        [ 0, 10],\n",
       "        [ 0,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Data have to same size or do any other things \n",
    "# make batch with same size text and random all batchs\n",
    "# or use pre embadding not post embadding\n",
    "\n",
    "def add_startToken(texts, start_token='*'):\n",
    "    return [start_token + text for text in texts]\n",
    "def add_endToken(texts, end_token='^'):\n",
    "    return [text + end_token for text in texts]\n",
    "\n",
    "def preprocesser(texts: list[list], prePadding=True, vocab=eng_vocab, startToken=False, endToken=False, batch_first=False):\n",
    "    if startToken:\n",
    "        texts = add_startToken(texts)\n",
    "    if endToken:\n",
    "        texts = add_endToken(texts)\n",
    "    # Convert characters to integers (ASCII - 97)\n",
    "    text_ints = [[vocab.index(c) for c in text] for text in texts]\n",
    "    # Apply pre-padding to each sequence\n",
    "    if prePadding:\n",
    "        max_length = max(len(seq) for seq in text_ints)\n",
    "        padded_seqs = pad_sequence([torch.cat([torch.zeros(max_length - len(seq), dtype=torch.int64), torch.LongTensor(seq)]) for seq in text_ints], batch_first=True)\n",
    "    else:\n",
    "        padded_seqs = pad_sequence([torch.LongTensor(seq) for seq in text_ints], batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_seqs.to(device=device) if batch_first else padded_seqs.T.to(device=device)\n",
    "\n",
    "\n",
    "preprocesser(['hiir', 'laksfffh'], startToken=True, endToken=True, prePadding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1],\n",
       "        [ 2, 17],\n",
       "        [ 0, 50],\n",
       "        [ 0,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesser(['', 'का'], vocab=hin_vocab, prePadding=False, startToken=True, endToken=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244381/244381 [00:04<00:00, 52505.21it/s]\n",
      "100%|█████████▉| 472/473 [00:08<00:00, 56.87it/s]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, batch_size):\n",
    "        dataset = []\n",
    "        # with open('dataset_hinglish2hindi.txt', 'r', encoding='utf-8') as f:\n",
    "        with open('preprocessedHindi2Hinglish.txt', 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f.readlines()):\n",
    "                stop = False\n",
    "                for i in line:\n",
    "                    if i not in chr_eng+chr_hin+['\\t', '\\n']: \n",
    "                        stop = True\n",
    "                        break\n",
    "                if stop: continue\n",
    "                dataset.append(line.split()[::-1])\n",
    "        dataset.sort(key=lambda x: len(x[0]))\n",
    "        \n",
    "        batched = []\n",
    "        length = len(dataset)\n",
    "        for i in tqdm(range(0, length, batch_size)):\n",
    "            if i+batch_size>length: break\n",
    "            batched.append(self.custom_collate_fn(dataset[i:i+batch_size]))\n",
    "\n",
    "        self.dataset = batched\n",
    "        \n",
    "    @staticmethod\n",
    "    def custom_collate_fn(batch):\n",
    "        x = []\n",
    "        y = []\n",
    "        for ix, iy in batch:\n",
    "            x.append(''.join([i for i in ix.lower() if i in chr_eng]))\n",
    "            y.append(''.join([i for i in iy.lower() if i in chr_hin]))\n",
    "        x = preprocesser(x, startToken=True, endToken=True, prePadding=False)\n",
    "        y = preprocesser(y, vocab=hin_vocab, prePadding=False, startToken=True, endToken=True)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single sequence and its label\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader with batch size 64\n",
    "batch_size = 512\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape - Sequences: torch.Size([8, 512]) Labels: torch.Size([17, 512])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(custom_dataset, shuffle=True, batch_size=1)\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    sequences, labels = batch\n",
    "    sequences = sequences.squeeze(0)\n",
    "    labels = labels.squeeze(0)\n",
    "    print(\"Batch Shape - Sequences:\", sequences.shape, \"Labels:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    sequences, labels = batch\n",
    "    sequences = sequences.squeeze(0)\n",
    "    labels = labels.squeeze(0)\n",
    "    # slow training process very much so improve this or remove custom_collate_fn and set it into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    \n",
    "def bleu(data, model):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for (src,trg)  in tqdm(data):\n",
    "        try:\n",
    "            prediction = translate_sentence(model, src)\n",
    "            targets.append([trg])\n",
    "            outputs.append(prediction)\n",
    "        except: pass\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "def translate_sentence(model, text, max_length=50):\n",
    "    result = []\n",
    "    for word in text.split(' '):\n",
    "        x = preprocesser([word], startToken=True, vocab=eng_vocab, endToken=True)\n",
    "        stopIdx = hin_vocab.index(TOKEN_EOS)\n",
    "        outputs = []\n",
    "        for i in range(max_length):\n",
    "            y = preprocesser([''.join(outputs)], startToken=True, vocab=hin_vocab, endToken=False)\n",
    "            # print(y)\n",
    "            with torch.no_grad():\n",
    "                output = model(x, y)\n",
    "            best_guess = output.argmax(2)[-1, :].item()\n",
    "            # print(best_guess)\n",
    "            if best_guess == stopIdx: break\n",
    "            outputs.append(hin_vocab[best_guess])\n",
    "        result.append(''.join(outputs))\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "        useOneHotEncoder=False\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        if useOneHotEncoder:\n",
    "            embedding_size = max([src_vocab_size, max_len, trg_vocab_size])\n",
    "            embedding_size = max([src_vocab_size, max_len, trg_vocab_size])\n",
    "            embedding_size += embedding_size%8\n",
    "            print('Embedding size : ', embedding_size)\n",
    "            self.dropout = lambda x: x\n",
    "            self.src_word_embedding = lambda x: F.one_hot(x, embedding_size).float()\n",
    "            self.src_position_embedding = lambda x: F.one_hot(x, embedding_size).float()\n",
    "            self.trg_word_embedding = lambda x: F.one_hot(x, embedding_size).float()\n",
    "            self.trg_position_embedding = lambda x: F.one_hot(x, embedding_size).float()\n",
    "        else:\n",
    "            self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "            self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "            self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "            self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 3e-4\n",
    "batch_size = 512*4\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(eng_vocab)\n",
    "trg_vocab_size = len(hin_vocab)\n",
    "embedding_size = 128\n",
    "num_heads = 8\n",
    "assert embedding_size%num_heads == 0\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dropout = 0.10\n",
    "max_len = 50\n",
    "forward_expansion = 4\n",
    "src_pad_idx = eng_vocab.index(TOKEN_SOS)\n",
    "\n",
    "src_pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244381/244381 [00:04<00:00, 54056.53it/s]\n",
      " 99%|█████████▉| 118/119 [00:08<00:00, 14.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(batch_size)\n",
    "\n",
    "train_iterator = DataLoader(custom_dataset, batch_size=1, shuffle=True) # batch_size == 1 as we set it in data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17, 2048]), torch.Size([12, 2048]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch_idx, (batch_src, batch_trg) in enumerate(train_iterator):\n",
    "    batch_src = batch_src.squeeze(0)\n",
    "    batch_trg = batch_trg.squeeze(0)\n",
    "    break\n",
    "batch_trg.shape, batch_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size = None\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = hin_vocab.index(TOKEN_PAD)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(filename=f'model\\\\hinglish2hindi_epoch-50.pth.tar', model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 50]\n",
      "Translated example sentence: \n",
      " गझक़ँपसँँदझक़रजपररँक़ँँँँरईँड़चरझक़ँँखपरईऊठँँपृृृृृक़ँचओ गँँँबईँँदझढ़पररईँँक़ँँँँरईँड़चरँँँँहँहईऊृृठपृृृृृृक़रओ गग़ँँकँँँबूकठततपरजपृतजँरईँड़तरछजज़तहकहगरईँबगतजँगँँँचर गझक़ँपसँछदझकक़ररईँपक़ँँबकँक़ँड़चरछजृक़ँपरईसक़ँबगँपृृृृक़ँओ गठसँकपससढ़झकठअरईड़ँपृृृृृृृृृृृृृअड़कहईऊृृठपृृृृृृक़ँठ गठसँकजँड़ढ़झढ़ठततठरजपृृृकँगँड़तठअजजँहकहईसईँँहतजँगँँँहत\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 1 / 50]\n",
      "Translated example sentence: \n",
      " मरार नामा लाक कुमर सिसियिया हि\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्श कुमर सिसोडिया हाइ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमार सिसोडिया हैई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमार सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 11 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:09<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 21 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 31 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:09<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 41 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:11<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:09<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:08<00:00, 13.29it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"mera naam laksh kumar sisodiya hai\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, max_length=max_len\n",
    "    )\n",
    "# \n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    total_length = train_iterator.__len__()\n",
    "\n",
    "    for batch_idx, (batch_src, batch_trg) in tqdm(enumerate(train_iterator), total=total_length):\n",
    "        # removed as we fix this in own dataset if batch_idx == total_length - 1: break # stop 1 before as something is wrong for small batch size\n",
    "        batch_src = batch_src.squeeze(0)\n",
    "        batch_trg = batch_trg.squeeze(0)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch_src.to(device)\n",
    "        target = batch_trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    \n",
    "    if save_model and (epoch+1)%10==0 and epoch!=0:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=f'model\\\\hinglish2hindi_epoch-{epoch+1}.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),    \n",
    "}\n",
    "save_checkpoint(checkpoint, filename=f'model\\\\hinglish2hindi_epoch-50.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(filename=f'model\\\\hinglish2hindi_epoch-50.pth.tar', model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "translated_sentence = translate_sentence(\n",
    "    model, \"mera naam laksh kumar sisodiya hai\", max_length=50\n",
    ")\n",
    "\n",
    "print(f\"Translated example sentence: \\n {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('dataset.txt', 'r', encoding='utf') as f:\n",
    "    for line in f.readlines():\n",
    "        lines.append(line)\n",
    "random.shuffle(lines)\n",
    "test_data = []\n",
    "for i in lines:\n",
    "    test_data.append(i.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:15<00:00, 32.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 59.48\n"
     ]
    }
   ],
   "source": [
    "# running on entire test data takes a while\n",
    "score = bleu(test_data[1:500], model)\n",
    "print(f\"Bleu score {score * 100:.2f}\") # EPOCH 10 -> 54.79 Now at EPOCH 110 -> 61.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'तुम क्या कर रहे हो'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "sentence = \"tum kya kar rahe ho\"\n",
    "translate_sentence(\n",
    "    model, sentence, max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence   modelOutput    real\n",
      "\n",
      "yaseelaa   => यसीला      => यशीला\n",
      "of         => ऑफ         => की\n",
      "fuller     => फुलर       => फुलर\n",
      "lutari     => लुटरी      => लुटारी\n",
      "hum        => हुम        => हम\n",
      "enosh      => एनओश       => एनोश\n",
      "mukam      => मुकम       => मुकाम\n",
      "purvon     => पूर्वों    => पुर्वो\n",
      "elda       => एलडा       => एल्दा\n",
      "king       => किंग       => राजा\n",
      "malika     => मलिका      => मलिका\n",
      "veer       => वीर        => वीर\n",
      "van        => वन         => वैन\n",
      "keladhan   => केलाधन     => कैलाघन\n",
      "charkhi    => चरखी       => चरखी\n",
      "part       => पार्ट      => ओर\n",
      "nigtingale => निग्तिंगले => नाइटेंगल\n",
      "varn       => वर्ण       => वर्न\n",
      "art        => अर्त       => आर्ट\n",
      "reservoir  => रिजरवोइर   => रिज़रवायर\n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "real = ''\n",
    "\n",
    "for _ in range(20):\n",
    "    line = random.choice(lines)\n",
    "    inp, tar = line.split()\n",
    "    sentence += inp+' '\n",
    "    real += tar+' '\n",
    "\n",
    "sentence = sentence.strip()\n",
    "real = real.strip()\n",
    "\n",
    "out = translate_sentence(model, sentence).split()\n",
    "\n",
    "sentence = sentence.split()\n",
    "real = real.split()\n",
    "max_sentence = max([len(i) for i in sentence])\n",
    "\n",
    "print(f\"sentence{' '*(max_sentence-len('sentence'))} modelOutput    real\\n\")\n",
    "\n",
    "for i, o in enumerate(out):\n",
    "    print(f\"{sentence[i]}{' '*(max_sentence-len(sentence[i]))} => {o:<10} => {real[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying with oneHot for educational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 3e-4\n",
    "batch_size = 512*4\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(eng_vocab)\n",
    "trg_vocab_size = len(hin_vocab)\n",
    "num_heads = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dropout = 0.10\n",
    "max_len = 72\n",
    "forward_expansion = 4\n",
    "src_pad_idx = eng_vocab.index(TOKEN_SOS)\n",
    "\n",
    "src_pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 71)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size, trg_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size :  72\n"
     ]
    }
   ],
   "source": [
    "embedding_size = max([src_vocab_size, max_len, trg_vocab_size])\n",
    "embedding_size = max([src_vocab_size, max_len, trg_vocab_size])\n",
    "print('Embedding size : ', embedding_size)\n",
    "assert embedding_size%num_heads == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size :  72\n"
     ]
    }
   ],
   "source": [
    "# embedding_size = None\n",
    "modelOneHot = Transformer(\n",
    "    None,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    "    useOneHotEncoder=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(modelOneHot.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = hin_vocab.index(TOKEN_PAD)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(filename=f'model\\\\hinglish2hindi_oneHot_epoch-50.pth.tar', model=modelOneHot, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 50]\n",
      "Translated example sentence: \n",
      " ॥ ॥स दऋफभसए  ॥॥थॅ ॥ ॥रररऋफ॥ ॥   ऋस टघ॥थवरदऋरस    रघ  दई    ॥रससदथरदसदस ख़ ॥ ॥थऋञरृयदथऋसऋरऋफ॥रई रररऋफ॥ ॥थररऋऋ थवोवढ़रदऋररअदस रई  दयद   ॥रससदथरदथदस ख़ ॥ ऑस ट॥॥ दथऋ ॥रयऑट॥ ॥रररऋ दईफ़॥ररदई टपदथ*॥फररट॥  दयद  दयद   ॥रदथदथरदएदऋरर ॥॥सस दथ॥ दथऋ ॥रऋफ॥रई रररऋफ॥ ॥थररऋऋञफररऋफ॥फरऋएख़दस ॥  ख़दई   ॥॥रससदऋरदऋफ॥फर   ट॥ ट॥  टदऋ ॥रऋस ॥  रररऋ टदथऋररऋस टदोवढ़रदऋरट॥   रट  दयद   ॥रटटद रद दस र ॥ै॥रञदऋ॥ दथऋञृदऋख़ञृख़ञृदईफ़ृदईृख़दईदईफ़सदोञष॥फभबञृदऋञृदऋसदईदोञ ख़ञष॥ोञरदऋदऋरर\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 50]\n",
      "Translated example sentence: \n",
      " रमर ममाा स्स रार सससस माला\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 / 50]\n",
      "Translated example sentence: \n",
      " मर्र मान लाला कुर सिसिय्स हा\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 / 50]\n",
      "Translated example sentence: \n",
      " मरर नाम लास कुर्र सियियो हा\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 / 50]\n",
      "Translated example sentence: \n",
      " मरार नाम लक्स कुमर सिस्योडी हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5 / 50]\n",
      "Translated example sentence: \n",
      " मरा नाम लक्श कुमर सिसोया हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्श कुमर सिसोडी हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्श कुमर सिसोडी हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्श कुमर सिसोडी हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्श कुमर सिसोडी हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 10 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडीया हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडीया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडीया हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडीया हाई\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडीया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडीया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 20 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 30 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "[Epoch 40 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोदिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49 / 50]\n",
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:07<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "sentence = \"mera naam laksh kumar sisodiya hai\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    modelOneHot.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        modelOneHot, sentence, max_length=max_len\n",
    "    )\n",
    "# \n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    modelOneHot.train()\n",
    "    losses = []\n",
    "    \n",
    "    total_length = train_iterator.__len__()\n",
    "\n",
    "    for batch_idx, (batch_src, batch_trg) in tqdm(enumerate(train_iterator), total=total_length):\n",
    "        # removed as we fix this in own dataset if batch_idx == total_length - 1: break # stop 1 before as something is wrong for small batch size\n",
    "        batch_src = batch_src.squeeze(0)\n",
    "        batch_trg = batch_trg.squeeze(0)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch_src.to(device)\n",
    "        target = batch_trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = modelOneHot(inp_data, target[:-1, :])\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(modelOneHot.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    \n",
    "    if save_model and (epoch+1)%10==0 and epoch!=0:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": modelOneHot.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=f'model\\\\hinglish2hindi_oneHot_epoch-{epoch+1}.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": modelOneHot.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),    \n",
    "}\n",
    "save_checkpoint(checkpoint, filename=f'model\\\\hinglish2hindi_oneHot_epoch-50.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(filename=f'model\\\\hinglish2hindi_oneHot_epoch-50.pth.tar', model=modelOneHot, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example sentence: \n",
      " मेरा नाम लक्ष कुमर सिसोडिया है\n"
     ]
    }
   ],
   "source": [
    "modelOneHot.eval()\n",
    "translated_sentence = translate_sentence(\n",
    "    modelOneHot, \"mera naam laksh kumar sisodiya hai\", max_length=50\n",
    ")\n",
    "\n",
    "print(f\"Translated example sentence: \\n {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('dataset.txt', 'r', encoding='utf') as f:\n",
    "    for line in f.readlines():\n",
    "        lines.append(line)\n",
    "random.shuffle(lines)\n",
    "test_data = []\n",
    "for i in lines:\n",
    "    test_data.append(i.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:17<00:00, 28.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 49.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# running on entire test data takes a while\n",
    "score = bleu(test_data[1:500], modelOneHot)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'तुम क्या कर रहे हो'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelOneHot.eval()\n",
    "sentence = \"tum kya kar rahe ho\"\n",
    "translate_sentence(\n",
    "    modelOneHot, sentence, max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence  modelOutput    real\n",
      "\n",
      "maria     => मरिया      => मारिया\n",
      "abdul     => अबुल       => अब्दुल\n",
      "adimurty  => अदिमुर्त्य => आदिमूर्ति\n",
      "pahala    => पहला       => पहला\n",
      "tata      => तता        => टाटा\n",
      "law       => लव         => कानून\n",
      "chirag    => चिराग      => चिराग़\n",
      "gulabganj => गुलबगंज    => गुलाबगंज\n",
      "museum    => मुसेम      => म्युज़ियम\n",
      "koen      => कोएन       => कोएन\n",
      "karter    => कर्तर      => कार्टर\n",
      "babu      => बबू        => बाबू\n",
      "baspist   => बसपिस्ट    => बैपटिस्ट\n",
      "gray      => ग्रय       => कंबरलैंड\n",
      "ramesh    => रमेश       => रमेश\n",
      "durg      => दुर्ग      => दुर्ग\n",
      "kanada    => कनदा       => कनाडा\n",
      "yvonne    => यूनने      => वॉन्नी\n",
      "batuque   => बतुकुए     => बैट्यूक\n",
      "carolina  => करोलिना    => कैरोलिना\n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "real = ''\n",
    "\n",
    "for _ in range(20):\n",
    "    line = random.choice(lines)\n",
    "    inp, tar = line.split()\n",
    "    sentence += inp+' '\n",
    "    real += tar+' '\n",
    "\n",
    "sentence = sentence.strip()\n",
    "real = real.strip()\n",
    "\n",
    "out = translate_sentence(modelOneHot, sentence).split()\n",
    "\n",
    "sentence = sentence.split()\n",
    "real = real.split()\n",
    "max_sentence = max([len(i) for i in sentence])\n",
    "\n",
    "print(f\"sentence{' '*(max_sentence-len('sentence'))} modelOutput    real\\n\")\n",
    "\n",
    "for i, o in enumerate(out):\n",
    "    print(f\"{sentence[i]}{' '*(max_sentence-len(sentence[i]))} => {o:<10} => {real[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
